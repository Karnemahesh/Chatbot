{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:21.410804Z","iopub.execute_input":"2025-03-31T08:22:21.411213Z","iopub.status.idle":"2025-03-31T08:22:21.819582Z","shell.execute_reply.started":"2025-03-31T08:22:21.411174Z","shell.execute_reply":"2025-03-31T08:22:21.818132Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:21.822184Z","iopub.execute_input":"2025-03-31T08:22:21.822602Z","iopub.status.idle":"2025-03-31T08:22:22.694785Z","shell.execute_reply.started":"2025-03-31T08:22:21.822574Z","shell.execute_reply":"2025-03-31T08:22:22.693682Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:22.697048Z","iopub.execute_input":"2025-03-31T08:22:22.697817Z","iopub.status.idle":"2025-03-31T08:22:22.761254Z","shell.execute_reply.started":"2025-03-31T08:22:22.697770Z","shell.execute_reply":"2025-03-31T08:22:22.760196Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:22.762285Z","iopub.execute_input":"2025-03-31T08:22:22.762896Z","iopub.status.idle":"2025-03-31T08:22:22.928656Z","shell.execute_reply.started":"2025-03-31T08:22:22.762865Z","shell.execute_reply":"2025-03-31T08:22:22.927596Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:22.929949Z","iopub.execute_input":"2025-03-31T08:22:22.930391Z","iopub.status.idle":"2025-03-31T08:22:25.227909Z","shell.execute_reply.started":"2025-03-31T08:22:22.930310Z","shell.execute_reply":"2025-03-31T08:22:25.226972Z"}},"outputs":[{"name":"stdout","text":"Okay, imagine you have a really smart puppy!  \n\nNormally, you have to teach a puppy everything.  \"Sit,\" \"Stay,\" \"Fetch.\" And you have to tell it over and over and over!\n\nAI is like a puppy that can learn all those things, but WAY faster and sometimes even by itself! \n\nWe teach AI by giving it lots and lots of examples.  Like, if we want AI to recognize pictures of cats, we show it millions of pictures of cats!  Then, the AI figures out what makes a cat a cat, like pointy ears and whiskers.\n\nSo, when you show it a NEW picture, it can say, \"Hey! That looks like a cat!\"\n\nThat's AI!  It's a computer that can learn and solve problems like a really smart puppy, but with information instead of treats!\n\nThink about your favorite video game. The bad guys know how to chase you, right?  That's probably AI telling them where to go and how to find you!\n\nAI is used for lots of things, like:\n\n*   **Talking robots:**  Like Siri or Alexa, they can answer your questions.\n*   **Self-driving cars:**  They can see the road and drive you around safely.\n*   **Making recommendations:** Like when YouTube suggests videos you might like.\n*   **Doctors:** Helping them find diseases faster!\n\nAI is still learning and growing, just like a real puppy!  And someday, it might be able to do things we can't even imagine yet!\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:25.228813Z","iopub.execute_input":"2025-03-31T08:22:25.229219Z","iopub.status.idle":"2025-03-31T08:22:25.236974Z","shell.execute_reply.started":"2025-03-31T08:22:25.229176Z","shell.execute_reply":"2025-03-31T08:22:25.236032Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, imagine you have a really smart puppy!  \n\nNormally, you have to teach a puppy everything.  \"Sit,\" \"Stay,\" \"Fetch.\" And you have to tell it over and over and over!\n\nAI is like a puppy that can learn all those things, but WAY faster and sometimes even by itself! \n\nWe teach AI by giving it lots and lots of examples.  Like, if we want AI to recognize pictures of cats, we show it millions of pictures of cats!  Then, the AI figures out what makes a cat a cat, like pointy ears and whiskers.\n\nSo, when you show it a NEW picture, it can say, \"Hey! That looks like a cat!\"\n\nThat's AI!  It's a computer that can learn and solve problems like a really smart puppy, but with information instead of treats!\n\nThink about your favorite video game. The bad guys know how to chase you, right?  That's probably AI telling them where to go and how to find you!\n\nAI is used for lots of things, like:\n\n*   **Talking robots:**  Like Siri or Alexa, they can answer your questions.\n*   **Self-driving cars:**  They can see the road and drive you around safely.\n*   **Making recommendations:** Like when YouTube suggests videos you might like.\n*   **Doctors:** Helping them find diseases faster!\n\nAI is still learning and growing, just like a real puppy!  And someday, it might be able to do things we can't even imagine yet!\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:25.238109Z","iopub.execute_input":"2025-03-31T08:22:25.238496Z","iopub.status.idle":"2025-03-31T08:22:25.679177Z","shell.execute_reply.started":"2025-03-31T08:22:25.238462Z","shell.execute_reply":"2025-03-31T08:22:25.678214Z"}},"outputs":[{"name":"stdout","text":"Hello Zlork! It's nice to meet you. How can I help you today?\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:25.683834Z","iopub.execute_input":"2025-03-31T08:22:25.684154Z","iopub.status.idle":"2025-03-31T08:22:27.008813Z","shell.execute_reply.started":"2025-03-31T08:22:25.684109Z","shell.execute_reply":"2025-03-31T08:22:27.008001Z"}},"outputs":[{"name":"stdout","text":"Okay, here's an interesting fact about dinosaurs:\n\n**Some dinosaurs were probably brightly colored, like birds!**\n\nWhile we often picture dinosaurs as drab greens and browns, recent studies suggest many dinosaurs, especially feathered ones, possessed vibrant and complex color patterns. Scientists have found fossilized melanosomes (pigment-bearing structures) in dinosaur feathers, which allow them to reconstruct what colors they likely were. We're talking potential iridescent feathers, stripes, spots, and all sorts of dazzling displays!\n\nThis likely served various purposes, just like with birds today, including attracting mates, camouflage, and even species recognition. So, the next time you think of dinosaurs, imagine them in a riot of colors, not just dull earth tones!\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"response = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:27.010488Z","iopub.execute_input":"2025-03-31T08:22:27.010737Z","iopub.status.idle":"2025-03-31T08:22:27.540882Z","shell.execute_reply.started":"2025-03-31T08:22:27.010714Z","shell.execute_reply":"2025-03-31T08:22:27.539999Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Zlork.\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"for model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:27.541905Z","iopub.execute_input":"2025-03-31T08:22:27.542211Z","iopub.status.idle":"2025-03-31T08:22:27.573143Z","shell.execute_reply.started":"2025-03-31T08:22:27.542175Z","shell.execute_reply":"2025-03-31T08:22:27.572165Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.5-pro-exp-03-25\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/gemma-3-27b-it\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/aqa\nmodels/imagen-3.0-generate-002\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:27.574127Z","iopub.execute_input":"2025-03-31T08:22:27.574484Z","iopub.status.idle":"2025-03-31T08:22:27.608491Z","shell.execute_reply.started":"2025-03-31T08:22:27.574451Z","shell.execute_reply":"2025-03-31T08:22:27.607338Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent', 'countTokens'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:27.609508Z","iopub.execute_input":"2025-03-31T08:22:27.609890Z","iopub.status.idle":"2025-03-31T08:22:28.807295Z","shell.execute_reply.started":"2025-03-31T08:22:27.609843Z","shell.execute_reply":"2025-03-31T08:22:28.806404Z"}},"outputs":[{"name":"stdout","text":"## The Humble Olive: A Cornerstone of Modern Society\n\nThe olive, a humble fruit born of the Mediterranean basin, often overlooked amidst a plethora of exotic culinary offerings, plays a surprisingly crucial role in modern society. More than just a salty snack or a pizza topping, the olive and its transformative product, olive oil, are deeply intertwined with health, culture, economy, and sustainability, influencing the very fabric of modern life in profound and multifaceted ways.\n\nOne of the most significant contributions of the olive to modern society lies in its undeniable health benefits. Scientific research has consistently lauded the Mediterranean diet, of which olive oil is a cornerstone, for its association with reduced risk of cardiovascular disease, type 2 diabetes, certain types of cancer, and neurodegenerative disorders. The monounsaturated fats prevalent in olive oil, particularly oleic acid, contribute to improved cholesterol levels, reducing the risk of plaque buildup in arteries. Furthermore, olive oil is a rich source of antioxidants, including polyphenols like oleocanthal\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a short poem on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:28.808125Z","iopub.execute_input":"2025-03-31T08:22:28.808366Z","iopub.status.idle":"2025-03-31T08:22:29.759930Z","shell.execute_reply.started":"2025-03-31T08:22:28.808346Z","shell.execute_reply":"2025-03-31T08:22:29.758899Z"}},"outputs":[{"name":"stdout","text":"From ancient groves to modern plate,\nAn olive's worth we celebrate.\nIn oil it flows, a golden stream,\nA healthy choice, a vibrant dream.\n\nOn pizzas perched, in salads bright,\nA salty, tangy, pure delight.\nFrom tapenade to simple snack,\nIts earthy flavor calls us back.\n\nA symbol of peace, a culinary star,\nThe humble olive shines afar.\nSo raise a glass, or dip a bread,\nTo this small fruit, so richly fed.\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:29.761051Z","iopub.execute_input":"2025-03-31T08:22:29.761386Z","iopub.status.idle":"2025-03-31T08:22:30.939846Z","shell.execute_reply.started":"2025-03-31T08:22:29.761362Z","shell.execute_reply":"2025-03-31T08:22:30.938860Z"}},"outputs":[{"name":"stdout","text":"Purple\n -------------------------\nTeal\n -------------------------\nCerulean\n -------------------------\nAzure.\n -------------------------\nTurquoise\n -------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:30.940847Z","iopub.execute_input":"2025-03-31T08:22:30.941213Z","iopub.status.idle":"2025-03-31T08:22:32.182774Z","shell.execute_reply.started":"2025-03-31T08:22:30.941186Z","shell.execute_reply":"2025-03-31T08:22:32.181824Z"}},"outputs":[{"name":"stdout","text":"Azure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=0.95,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:22:32.183632Z","iopub.execute_input":"2025-03-31T08:22:32.183974Z","iopub.status.idle":"2025-03-31T08:24:17.328579Z","shell.execute_reply.started":"2025-03-31T08:22:32.183934Z","shell.execute_reply":"2025-03-31T08:24:17.327511Z"}},"outputs":[{"name":"stdout","text":"Clementine wasn't your average ginger tabby. Oh, she enjoyed sunbeams and napping in cardboard boxes as much as the next feline, but beneath her fluffy exterior beat the heart of an adventurer. The stories the pigeons whispered outside her window, tales of bustling markets and shimmering rivers, fuelled a burning desire to explore beyond the confines of her cozy little cottage.\n\nOne crisp autumn morning, the window was ajar. A siren song of woodsmoke and fallen leaves drifted in, and Clementine knew. This was her day. With a silent prayer to Bast, the cat goddess, she leaped onto the sill and into the unknown.\n\nThe first hour was a kaleidoscope of new smells. The pungent tang of fertilizer from old Mrs. Higgins' garden, the sweet decay of rotting apples under a forgotten tree. She navigated with a practiced swagger, tail held high, dodging the wheels of bicycles and the clumsy feet of oblivious humans.\n\nHer journey led her to the edge of the Whispering Woods, a place her humans had always warned her about. \"Stay away from the woods, Clementine! Wild things live there!\" But fear was a stranger to her adventurous spirit. She plunged into the cool, dappled shade.\n\nThe woods were a symphony of rustling leaves and chirping insects. Clementine stalked through undergrowth thick with ferns, her senses on high alert. She startled a squirrel, who chattered insults from a high branch, and nearly stepped on a sleepy hedgehog who grumbled and curled into a spiky ball.\n\nDeep within the woods, she came across a small, moss-covered stream. A tiny, silver fish darted through the water, tantalizingly close. Clementine crouched, her hunting instincts kicking in. She tensed, preparing to pounce when…\n\n\"Psst! Hey, ginger!\"\n\nClementine whipped around. Perched on a gnarled root was a badger, his nose twitching with amusement. \"Trying your luck, are ya?\" he chuckled, his voice surprisingly gentle.\n\nClementine, flustered, puffed up her fur. \"I was just… admiring the scenery.\"\n\nThe badger winked. \"Sure you were. Listen, if you want a proper adventure, you should follow this stream upstream. It leads to Old Man Willow's Wishing Pool. They say if you make a wish there, it comes true.\"\n\nOld Man Willow's Wishing Pool? Clementine's ears perked up. She forgot all about the silver fish. \"A Wishing Pool? Really?\"\n\n\"Really,\" the badger said, hopping down from the root. \"But be warned, the path is treacherous. Watch out for the Grumpy Goblins who guard the bridge.\"\n\nWith a nod of thanks to the badger, Clementine followed the stream, her heart pounding with anticipation. The path grew narrower and steeper, forcing her to scramble over rocks and under low-hanging branches.\n\nFinally, she reached the bridge. And there they were. Two Grumpy Goblins, short and squat, with noses like squashed plums and eyes like angry beetles. They blocked the way with crude spears fashioned from branches.\n\n\"Halt!\" croaked the first goblin. \"You shall not pass!\"\n\nClementine, remembering a trick she learned from watching the pigeons, pretended to yawn. Then, in a voice dripping with boredom, she said, \"Oh, not another one of these charades. I've seen it all before. Spear? Please. I could take you both in my sleep.\"\n\nThe goblins, clearly not used to being challenged, exchanged confused glances. Clementine seized the moment. She leaped over the bridge, landing gracefully on the other side. The goblins shrieked in outrage, but she was already gone.\n\nAnd then, she saw it. The Wishing Pool. A small, circular pool of water reflecting the sky like a shimmering mirror. Old Man Willow, a massive, ancient tree, wept weeping willow branches over the pool, casting it in a mystical light.\n\nClementine approached the pool cautiously. She dipped a paw in the cool water. What to wish for? More sunbeams? An endless supply of tuna?\n\nShe closed her eyes, took a deep breath, and made her wish. Not for material things, but for the courage to continue exploring, to embrace the unknown, and to always find her way back home.\n\nAs the sun began to set, casting long shadows through the woods, Clementine turned back. The journey home was easier, her steps lighter. She had faced her fears, encountered strange creatures, and found the magic that lay within herself.\n\nBack in her cozy cottage, curled up on her favourite cushion, Clementine purred contentedly. The adventure had been exhilarating, but there was no place quite like home. And tomorrow, she knew, the pigeons would have some exciting stories to hear.\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:17.329670Z","iopub.execute_input":"2025-03-31T08:24:17.330308Z","iopub.status.idle":"2025-03-31T08:24:17.688305Z","shell.execute_reply.started":"2025-03-31T08:24:17.330268Z","shell.execute_reply":"2025-03-31T08:24:17.687415Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:17.689104Z","iopub.execute_input":"2025-03-31T08:24:17.689347Z","iopub.status.idle":"2025-03-31T08:24:18.182881Z","shell.execute_reply.started":"2025-03-31T08:24:17.689325Z","shell.execute_reply":"2025-03-31T08:24:18.181816Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:18.184025Z","iopub.execute_input":"2025-03-31T08:24:18.184317Z","iopub.status.idle":"2025-03-31T08:24:18.190507Z","shell.execute_reply.started":"2025-03-31T08:24:18.184294Z","shell.execute_reply":"2025-03-31T08:24:18.189197Z"}},"outputs":[{"name":"stdout","text":"Sentiment.POSITIVE\n<enum 'Sentiment'>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:18.191515Z","iopub.execute_input":"2025-03-31T08:24:18.191876Z","iopub.status.idle":"2025-03-31T08:24:18.679908Z","shell.execute_reply.started":"2025-03-31T08:24:18.191842Z","shell.execute_reply":"2025-03-31T08:24:18.679021Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:18.680850Z","iopub.execute_input":"2025-03-31T08:24:18.681217Z","iopub.status.idle":"2025-03-31T08:24:19.183988Z","shell.execute_reply.started":"2025-03-31T08:24:18.681184Z","shell.execute_reply":"2025-03-31T08:24:19.182983Z"}},"outputs":[{"name":"stdout","text":"{\n  \"size\": \"large\",\n  \"ingredients\": [\"apple\", \"chocolate\"],\n  \"type\": \"dessert\"\n}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:19.185041Z","iopub.execute_input":"2025-03-31T08:24:19.185360Z","iopub.status.idle":"2025-03-31T08:24:19.544958Z","shell.execute_reply.started":"2025-03-31T08:24:19.185336Z","shell.execute_reply":"2025-03-31T08:24:19.543866Z"}},"outputs":[{"name":"stdout","text":"48\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:19.546030Z","iopub.execute_input":"2025-03-31T08:24:19.546380Z","iopub.status.idle":"2025-03-31T08:24:20.327285Z","shell.execute_reply.started":"2025-03-31T08:24:19.546344Z","shell.execute_reply":"2025-03-31T08:24:20.326382Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's how to solve the problem step-by-step:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, so they were 4 * 3 = 12 years old.\n\n2. **Calculate the age gap:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3. **Determine the partner's current age:** Since your partner is 8 years older than you, they are currently 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**\n"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:20.328307Z","iopub.execute_input":"2025-03-31T08:24:20.328635Z","iopub.status.idle":"2025-03-31T08:24:20.333558Z","shell.execute_reply.started":"2025-03-31T08:24:20.328599Z","shell.execute_reply":"2025-03-31T08:24:20.332673Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:20.334515Z","iopub.execute_input":"2025-03-31T08:24:20.334778Z","iopub.status.idle":"2025-03-31T08:24:20.906556Z","shell.execute_reply.started":"2025-03-31T08:24:20.334756Z","shell.execute_reply":"2025-03-31T08:24:20.905574Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the transformers NLP paper, then identify the authors and determine the youngest.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:20.910036Z","iopub.execute_input":"2025-03-31T08:24:20.910314Z","iopub.status.idle":"2025-03-31T08:24:21.753379Z","shell.execute_reply.started":"2025-03-31T08:24:20.910292Z","shell.execute_reply":"2025-03-31T08:24:21.752337Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI have the authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Now I need to find the youngest. I don't know the ages of any of these people. I'll search each individually to find their birthdates.\n\nAction 1\n<search>Ashish Vaswani</search>\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:21.754736Z","iopub.execute_input":"2025-03-31T08:24:21.755142Z","iopub.status.idle":"2025-03-31T08:24:30.639173Z","shell.execute_reply.started":"2025-03-31T08:24:21.755103Z","shell.execute_reply":"2025-03-31T08:24:30.638325Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the information available, the youngest author listed on the \"Attention is All You Need\" paper (the Transformer paper) is **Aidan N. Gomez**.\n\nWhile precise birthdates for all authors aren't readily and publicly available, Aidan N. Gomez was a student at the University of Toronto at the time of publication (2017). This generally implies he was younger than the other authors, who were primarily researchers at Google Brain and other established institutions.\n\nTherefore, **Aidan N. Gomez** is widely considered the youngest author on the Transformer paper."},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:30.640234Z","iopub.execute_input":"2025-03-31T08:24:30.640569Z","iopub.status.idle":"2025-03-31T08:24:31.630128Z","shell.execute_reply.started":"2025-03-31T08:24:30.640534Z","shell.execute_reply":"2025-03-31T08:24:31.629152Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  \"\"\"Calculates the factorial of a non-negative integer.\n\n  Args:\n    n: A non-negative integer.\n\n  Returns:\n    The factorial of n (n!), or 1 if n is 0.\n\n  Raises:\n    ValueError: If n is negative.\n  \"\"\"\n  if n < 0:\n    raise ValueError(\"Factorial is not defined for negative numbers.\")\n  if n == 0:\n    return 1\n  else:\n    result = 1\n    for i in range(1, n + 1):\n      result *= i\n    return result\n```\n"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from pprint import pprint\n\nconfig = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nfor part in response.candidates[0].content.parts:\n  pprint(part.to_json_dict())\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:31.631036Z","iopub.execute_input":"2025-03-31T08:24:31.631310Z","iopub.status.idle":"2025-03-31T08:24:33.805872Z","shell.execute_reply.started":"2025-03-31T08:24:31.631288Z","shell.execute_reply":"2025-03-31T08:24:33.804773Z"}},"outputs":[{"name":"stdout","text":"{'text': 'Okay, I can do that. First, I need to generate the first 14 odd '\n         'prime numbers. Prime numbers are numbers greater than 1 that are '\n         'only divisible by 1 and themselves. Since the question asks for '\n         '*odd* prime numbers, I should exclude the prime number 2.\\n'\n         '\\n'\n         'Then, after finding those numbers, I need to calculate their sum.\\n'}\n-----\n{'executable_code': {'code': 'def is_prime(n):\\n'\n                             '  \"\"\"Checks if a number is prime.\"\"\"\\n'\n                             '  if n <= 1:\\n'\n                             '    return False\\n'\n                             '  for i in range(2, int(n**0.5) + 1):\\n'\n                             '    if n % i == 0:\\n'\n                             '      return False\\n'\n                             '  return True\\n'\n                             '\\n'\n                             'odd_primes = []\\n'\n                             'num = 3\\n'\n                             'while len(odd_primes) < 14:\\n'\n                             '  if is_prime(num):\\n'\n                             '    odd_primes.append(num)\\n'\n                             '  num += 2\\n'\n                             '\\n'\n                             \"print(f'{odd_primes=}')\\n\"\n                             '\\n'\n                             'import numpy as np\\n'\n                             'sum_of_primes = np.sum(odd_primes)\\n'\n                             \"print(f'{sum_of_primes=}')\\n\",\n                     'language': 'PYTHON'}}\n-----\n{'code_execution_result': {'outcome': 'OUTCOME_OK',\n                           'output': 'odd_primes=[3, 5, 7, 11, 13, 17, 19, 23, '\n                                     '29, 31, 37, 41, 43, 47]\\n'\n                                     'sum_of_primes=np.int64(326)\\n'}}\n-----\n{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n         '31, 37, 41, 43, and 47.\\n'\n         '\\n'\n         'The sum of these numbers is 326.\\n'}\n-----\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:33.806976Z","iopub.execute_input":"2025-03-31T08:24:33.807302Z","iopub.status.idle":"2025-03-31T08:24:33.819096Z","shell.execute_reply.started":"2025-03-31T08:24:33.807277Z","shell.execute_reply":"2025-03-31T08:24:33.817986Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can do that. First, I need to generate the first 14 odd prime numbers. Prime numbers are numbers greater than 1 that are only divisible by 1 and themselves. Since the question asks for *odd* prime numbers, I should exclude the prime number 2.\n\nThen, after finding those numbers, I need to calculate their sum.\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef is_prime(n):\n  \"\"\"Checks if a number is prime.\"\"\"\n  if n <= 1:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\nodd_primes = []\nnum = 3\nwhile len(odd_primes) < 14:\n  if is_prime(num):\n    odd_primes.append(num)\n  num += 2\n\nprint(f'{odd_primes=}')\n\nimport numpy as np\nsum_of_primes = np.sum(odd_primes)\nprint(f'{sum_of_primes=}')\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```\nodd_primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=np.int64(326)\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.\n\nThe sum of these numbers is 326.\n"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes = sum(primes)\nprint(f'{primes=}')\nprint(f'{sum_of_primes=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:33.820335Z","iopub.execute_input":"2025-03-31T08:24:33.820701Z","iopub.status.idle":"2025-03-31T08:24:33.835106Z","shell.execute_reply.started":"2025-03-31T08:24:33.820665Z","shell.execute_reply":"2025-03-31T08:24:33.834128Z"}},"outputs":[{"name":"stdout","text":"primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=326\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:24:33.836072Z","iopub.execute_input":"2025-03-31T08:24:33.836332Z","iopub.status.idle":"2025-03-31T08:24:37.417532Z","shell.execute_reply.started":"2025-03-31T08:24:33.836311Z","shell.execute_reply":"2025-03-31T08:24:37.416443Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file, `git-prompt.sh`, is a Bash script designed to enhance your command-line prompt with information about the current Git repository.  Specifically, it customizes your prompt to show the branch name, the status of your working directory (e.g., staged changes, uncommitted changes, untracked files), and the relationship to the remote repository (ahead, behind, or diverged).\n\n**Here's a breakdown:**\n\n*   **What it is:** A script that modifies the appearance of your command-line prompt.\n*   **Why use it:**\n    *   **Git Status at a Glance:**  Provides immediate visibility into the status of your Git repository without needing to run `git status` manually every time.\n    *   **Improved Workflow:**  By displaying essential Git information, it helps you stay aware of your changes and promotes a smoother Git workflow.\n    *   **Customizable:** Allows you to tailor the prompt's appearance to suit your preferences (colors, symbols, displayed information).\n    *   **Virtualenv Support:** Supports virtualenv environments, showing the environment name in the prompt.\n*   **How it works:**\n    1.  **Defines Functions:** The script defines a set of functions to:\n        *   Determine Git repository status.\n        *   Format the prompt string.\n        *   Handle colors and themes.\n        *   Asynchronously fetch remote status.\n    2.  **Configuration:**  It reads configuration options from environment variables (e.g., `GIT_PROMPT_SHOW_UNTRACKED_FILES`, `GIT_PROMPT_THEME`).  It also looks for local `.bash-git-rc` config files in the git repo.\n    3.  **`setGitPrompt` function:** The core function that determines whether the current directory is a Git repository, gets the Git status, and constructs the new prompt string.\n    4.  **`PROMPT_COMMAND`:** It uses the `PROMPT_COMMAND` variable in Bash (or equivalent in Zsh) to execute the `setGitPrompt` function before each prompt is displayed. This ensures the prompt is updated dynamically.\n    5.  **Color and Themes:** It loads color definitions and themes from separate files, making it easy to customize the prompt's appearance.\n*   **How to install:**\n    1.  Source the file in your `.bashrc` or `.zshrc` file. `source /path/to/git-prompt.sh`\n    2.  Initialize the prompt: `gp_install_prompt`\n    3.  Optionally, configure other global or repo local `.bash-git-rc` variables to further configure its behavior.\n\nIn essence, `git-prompt.sh` is a helper tool that provides a more informative and Git-aware command-line experience.\n"},"metadata":{}}],"execution_count":32}]}